MLOps and Testing (عملیات هوش مصنوعی و آزمون)

این سند، رویه‌ها و ابزارهای مورد استفاده برای مدیریت چرخه حیات کامل مدل‌های هوش مصنوعی (MLOps) و تضمین کیفیت و عملکرد آن‌ها (Testing) در پلتفرم Vira AI را تعریف می‌کند.

۱. عملیات مدل‌های هوش مصنوعی (MLOps)

MLOps ترکیبی از توسعه نرم‌افزار (DevOps) و یادگیری ماشین (ML) است که هدف آن استقرار و نگهداری مدل‌ها در محیط تولید به صورت قابل اعتماد و کارآمد است.

۱.۱. ثبت و مدیریت مدل (Model Registry and Management)

Model Registry: باید یک مخزن متمرکز برای ذخیره، نسخه‌بندی (Versioning)، و ردیابی تمام مدل‌های آموزش دیده (شامل کدهای منبع، پیکربندی‌ها، و معیارهای عملکرد) وجود داشته باشد.

نسخه‌بندی: هر مدل باید دارای یک شناسه نسخه یکتا (Unique Version ID) باشد تا امکان Rollback و مقایسه مدل‌ها فراهم شود.

۱.۲. استقرار مدل (Model Deployment)

استقرار A/B (A/B Testing Deployment): استفاده از تکنیک‌هایی مانند Canary Deployments یا A/B Testing برای معرفی مدل‌های جدید به صورت تدریجی و ارزیابی عملکرد آن‌ها در ترافیک زنده قبل از استقرار کامل.

Endpointهای مقیاس‌پذیر: تمام مدل‌ها باید از طریق API Endpointهای مقیاس‌پذیر (با استفاده از Kubernetes) ارائه شوند.

۱.۳. مانیتورینگ عملکرد مدل (Model Performance Monitoring)

Drift Detection (تشخیص انحراف): نظارت مستمر بر داده‌های ورودی (Input Data) و توزیع پیش‌بینی‌های مدل (Prediction Output) برای تشخیص انحراف (Data Drift یا Model Drift) از داده‌های آموزشی اصلی.

Latency و Throughput: نظارت بر زمان پاسخگویی (Latency) و حجم پردازش (Throughput) مدل برای اطمینان از عملکرد بهینه در محیط تولید.

۲. استراتژی تست و اعتبارسنجی (Testing and Validation Strategy)

برای اطمینان از صحت و پایداری سرویس‌های هوش مصنوعی و زیرساخت، از سطوح مختلف تست استفاده می‌شود.

۲.۱. تست‌های زیرساخت و سرویس (Infrastructure and Service Tests)

Unit Tests: برای اعتبارسنجی منطق کسب‌وکار در میکروسرویس‌ها و توابع پردازش داده.

Integration Tests: برای تضمین ارتباط صحیح بین اجزای مختلف (مثلاً بین API Gateway و Prediction Service).

Load/Stress Tests: برای ارزیابی عملکرد سیستم تحت بار بالا و تعیین نقطه شکست (Breaking Point).

۲.۲. تست‌های مدل (Model-Specific Tests)

نوع تست

هدف

معیار ارزیابی

تست عملکرد (Performance)

اعتبارسنجی کیفیت پیش‌بینی مدل بر روی داده‌های آزمایشی (Test Dataset).

دقت (Accuracy)، F1 Score، Recall، ROC-AUC (بسته به نوع مدل).

تست پایداری (Robustness)

ارزیابی مدل با استفاده از داده‌های ورودی دستکاری‌شده یا پرت (Outliers) برای اطمینان از عدم شکست آن.

نسبت پاسخ‌های صحیح به داده‌های نویزی.

تست انصاف (Fairness)

ارزیابی عملکرد مدل برای زیرگروه‌های مختلف داده (مانند گروه‌های جمعیتی متفاوت) برای جلوگیری از تعصب (Bias).

برابری آماری (Statistical Parity) در نتایج پیش‌بینی.

تست رگرسیون (Regression)

اطمینان از اینکه مدل جدید یا به‌روزرسانی شده، عملکرد مدل قبلی را در موارد مهم کاهش نداده است.

مقایسه معیارهای کلیدی با نسخه قبلی مدل.

۳. بازخورد و یادگیری مجدد (Feedback and Retraining)

۳.۱. جمع‌آوری بازخورد (Feedback Loop)

باید مکانیزمی برای جمع‌آوری بازخورد از نتایج مدل در محیط تولید وجود داشته باشد (به عنوان مثال، ثبت صحت پیش‌بینی‌ها که توسط کاربران تایید شده‌اند).

۳.۲. اتوماسیون یادگیری مجدد (Automated Retraining)

در صورت تشخیص انحراف (Drift) یا کاهش عملکرد مدل زیر یک آستانه مشخص، فرآیند یادگیری مجدد مدل (Retraining) باید به صورت خودکار یا نیمه‌خودکار آغاز شود.

داده‌های جدید و با کیفیت بالا باید برای آموزش مدل جدید استفاده شوند و نتایج در Model Registry ثبت گردند.
